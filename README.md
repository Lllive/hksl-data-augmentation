# HKSL Financial Gloss LoRA Fine-Tuning Project

æœ¬é¡¹ç›®æ—¨åœ¨åŸºäº Qwen ç³»åˆ—æ¨¡å‹ï¼ˆQwen2.5-7B / Qwen3-VLï¼‰ï¼Œé€šè¿‡ LoRA å¾®è°ƒæŠ€æœ¯ï¼Œå®ç°**é‡‘èç²¤è¯­æ–‡æœ¬**åˆ°**é¦™æ¸¯æ‰‹è¯­ï¼ˆHKSLï¼‰Gloss è¯­æ³•**çš„è½¬æ¢ã€‚

## ğŸ–¥ï¸ ç¡¬ä»¶ä¸ç¯å¢ƒè¦æ±‚

- **GPU**: NVIDIA RTX 4090 (24GB VRAM)
- **OS**: Ubuntu / Linux
- **CUDA**: 12.1+ (æ¨è)
- **Python**: 3.10+

## ğŸ› ï¸ 1. ç¯å¢ƒå®‰è£… (Installation)

é¦–å…ˆå»ºç«‹ç‹¬ç«‹çš„ Conda ç¯å¢ƒï¼Œé˜²æ­¢ä¾èµ–å†²çªã€‚

```bash
# 1. åˆ›å»ºå¹¶æ¿€æ´»ç¯å¢ƒ
conda create -n llama_env python=3.10
conda activate llama_env

# 2. æ‹‰å– LLaMA-Factory ä»“åº“
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory

# 3. å®‰è£…ä¾èµ– (åŒ…å« metrics æ”¯æŒ)
pip install -e .[metrics]

# 4. å®‰è£… vLLM ç”¨äºåç»­æ¨ç†æœåŠ¡
pip install vllm

ğŸ“‚ 2. æ•°æ®å‡†å¤‡ (Data Preparation)
2.1 æ•°æ®æ ¼å¼

åœ¨ LLaMA-Factory/data ç›®å½•ä¸‹åˆ›å»º hksl_train.jsonã€‚æ•°æ®éœ€ç¬¦åˆ Alpaca æ ¼å¼ï¼š
json

[
  {
    "instruction": "å°†ä»¥ä¸‹é‡‘èç²¤è¯­æ–‡æœ¬è½¬æ¢ä¸ºç¬¦åˆé¦™æ¸¯æ‰‹è¯­(HKSL)è¯­æ³•çš„Glossæ ¼å¼ï¼Œä»…è¾“å‡ºç»“æœã€‚",
    "input": "æ’ç”ŸæŒ‡æ•°ä»Šæ—¥å‡å’—ä¸‰ç™¾ç‚¹ã€‚",
    "output": "ä»Šæ—¥ æ’ç”ŸæŒ‡æ•° å‡ ä¸‰ç™¾ ç‚¹"
  },
  {
    "instruction": "å°†ä»¥ä¸‹é‡‘èç²¤è¯­æ–‡æœ¬è½¬æ¢ä¸ºç¬¦åˆé¦™æ¸¯æ‰‹è¯­(HKSL)è¯­æ³•çš„Glossæ ¼å¼ï¼Œä»…è¾“å‡ºç»“æœã€‚",
    "input": "å¦‚æœä½ æƒ³å¼€æˆ·å£ï¼Œè¦å¸¦èº«ä»½è¯ã€‚",
    "output": "å¼€æˆ·å£ æƒ³ ä½ ï¼Ÿ èº«ä»½è¯ å¸¦ éœ€è¦"
  }
]

2.2 æ³¨å†Œæ•°æ®é›†

ç¼–è¾‘ LLaMA-Factory/data/dataset_info.jsonï¼Œåœ¨æ–‡ä»¶å¤´éƒ¨åŠ å…¥ä»¥ä¸‹æ³¨å†Œä¿¡æ¯ï¼š
json

"hksl_data": {
  "file_name": "hksl_train.json"
},

ğŸš€ 3. å¾®è°ƒè®­ç»ƒ (Fine-tuning)

ä½¿ç”¨ LLaMA-Factory çš„ WebUI è¿›è¡Œå¯è§†åŒ–è®­ç»ƒã€‚
å¯åŠ¨å‘½ä»¤
bash

export CUDA_VISIBLE_DEVICES=0
llamafactory-cli webui

å¯åŠ¨åè®¿é—®ï¼šhttp://localhost:7860
âš™ï¸ å…³é”®å‚æ•°é…ç½® (é’ˆå¯¹ RTX 4090 ä¼˜åŒ–)

è¯·åœ¨ WebUI ä¸­ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹å‚æ•°è®¾ç½®ï¼Œä»¥é˜²æ­¢ OOM (æ˜¾å­˜æº¢å‡º) å¹¶ä¿è¯æ•ˆæœï¼š

    Model Name: é€‰æ‹© Custom
    Model Path: /home/nvme_disk2/Miyeon_intern/lora/models/Qwen2.5-7B-Instruct (æ ¹æ®å®é™…è·¯å¾„å¡«å†™)
    Dataset: é€‰æ‹© hksl_data
    LoRA Rank: 16
    LoRA Alpha: 32
    Quantization bit: 4 (âš ï¸ å¿…é¡»é€‰ 4-bitï¼Œå¦åˆ™ 24G æ˜¾å­˜æ— æ³•åŠ è½½ 7B/8B æ¨¡å‹è¿›è¡Œè®­ç»ƒ)
    Batch Size: 1
    Gradient Accumulation: 8 (ç­‰æ•ˆ Batch Size = 8)
    Learning Rate: 2e-4 (æ¨è)

ç‚¹å‡» Start Training å¼€å§‹ç‚¼ä¸¹ã€‚
âš¡ 4. æ¨¡å‹æ¨ç†ä¸æœåŠ¡ (Inference & Serving)

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ vllm éƒ¨ç½²å…¼å®¹ OpenAI API çš„æ¨ç†æœåŠ¡ã€‚
å¯åŠ¨ API æœåŠ¡

å‡è®¾å¾®è°ƒåçš„æ¨¡å‹æƒé‡ä¿å­˜åœ¨ saves/Custom/lora/train_... æˆ–å·²åˆå¹¶å¯¼å‡ºè‡³ cut100 ç›®å½•ï¼š
bash

python -m vllm.entrypoints.openai.api_server \
    --model /home/nvme_disk2/Miyeon_intern/lora/models/Qwen2.5-7B-Instruct-cut100 \
    --served-model-name cut100 \
    --port 8000 \
    --trust-remote-code \
    --gpu-memory-utilization 0.6

ç›‘æ§æ˜¾å¡çŠ¶æ€

å¦å¼€ä¸€ä¸ªç»ˆç«¯ç›‘æ§è®­ç»ƒæˆ–æ¨ç†æ—¶çš„æ˜¾å­˜å ç”¨ï¼š
bash

watch -n 1 nvidia-smi

ğŸ“ ç›®å½•ç»“æ„è¯´æ˜
text

/home/nvme_disk2/Miyeon_intern/lora/
â”œâ”€â”€ data/                  # åŸå§‹è®­ç»ƒæ•°æ®
â”œâ”€â”€ LLaMA-Factory/         # è®­ç»ƒæ¡†æ¶æ ¸å¿ƒä»£ç 
â”‚   â”œâ”€â”€ data/              # éœ€è¦æ”¾å…¥ hksl_train.json çš„ä½ç½®
â”‚   â””â”€â”€ ...
â”œâ”€â”€ models/                # åŸºç¡€æ¨¡å‹å­˜æ”¾å¤„
â”‚   â”œâ”€â”€ Qwen2.5-7B-Instruct
â”‚   â””â”€â”€ Qwen3-VL-8B-Instruct
â””â”€â”€ saves/                 # è®­ç»ƒè¾“å‡ºçš„ LoRA æƒé‡
