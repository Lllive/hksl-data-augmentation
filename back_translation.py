import pandas as pd
import os
import logging
import json
import re
import difflib
from dotenv import load_dotenv
from openai import OpenAI
from tqdm import tqdm
from tenacity import retry, stop_after_attempt, wait_exponential
from sentence_transformers import SentenceTransformer, util
from concurrent.futures import ThreadPoolExecutor, as_completed

# ================= 0. åŠ è½½ç¯å¢ƒå˜é‡ =================
load_dotenv()

# ================= 1. é…ç½®æ—¥å¿— =================
logging.basicConfig(
    filename='multilang_backtrans_log.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# ================= 2. é…ç½®åŒºåŸŸ =================
# --- ğŸ› ï¸ æµ‹è¯•æ¨¡å¼å¼€å…³ ---
TEST_MODE = True       # True: ä»…è·‘å‡ æ¡æµ‹è¯•; False: è·‘å…¨é‡
TEST_LIMIT = 5         # æµ‹è¯•æ¨¡å¼ä¸‹å¤„ç†çš„ original æ•°æ®æ¡æ•°

INPUT_FILE = 'dataset_with_context_v2.csv'  # è¾“å…¥æ–‡ä»¶
OUTPUT_FILE = 'dataset_backtranslated.csv'  # è¾“å‡ºæ–‡ä»¶
REJECTED_FILE = 'backtrans_rejected.csv'

# --- é˜ˆå€¼è®¾ç½® ---
MAX_TEXT_SIMILARITY = 0.95     
MIN_SEMANTIC_SIMILARITY = 0.85 
MIN_LEN_RATIO = 0.6    
MAX_LEN_RATIO = 2.0    

MAX_WORKERS = 8  

if TEST_MODE:
    print(f"\nâš ï¸  æ³¨æ„ï¼šå½“å‰ä¸ºã€æµ‹è¯•æ¨¡å¼ã€‘ï¼Œä»…å¤„ç†å‰ {TEST_LIMIT} æ¡ 'original' æ•°æ®ã€‚")
    print(f"âš ï¸  å¹¶å‘æ•°å°†å¼ºåˆ¶è°ƒæ•´ä¸º 1ï¼Œä»¥ä¾¿åœ¨æ§åˆ¶å°æŸ¥çœ‹æ‰“å°è¾“å‡ºã€‚\n")
    MAX_WORKERS = 1
    OUTPUT_FILE = 'test_output_backtranslated.csv' # æµ‹è¯•ç»“æœå­˜åˆ°ä¸åŒæ–‡ä»¶

MODELS_CONFIG = [
    {
        "name": "qwen3-instruct",
        "url": os.getenv("OPENAI_API_URL_QWEN"), 
        "key": os.getenv("OPENAI_API_KEY_QWEN"),
        "model_id": "qwen-plus", 
        "params": {
            "temperature": 0.7,
            "max_tokens": 1500,
            "response_format": {"type": "json_object"}
        }
    },
]
active_config = MODELS_CONFIG[0]
client = OpenAI(api_key=active_config['key'], base_url=active_config['url'])

# ================= 3. æ¨¡å‹åŠ è½½ =================
print("â³ æ­£åœ¨åŠ è½½è¯­ä¹‰åŒ¹é…æ¨¡å‹...")
semantic_model = SentenceTransformer('shibing624/text2vec-base-chinese')
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

# ================= 4. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° =================

def calculate_text_similarity(s1, s2):
    return difflib.SequenceMatcher(None, s1, s2).ratio()

def calculate_semantic_similarity(s1, s2):
    embeddings1 = semantic_model.encode(s1, convert_to_tensor=True)
    embeddings2 = semantic_model.encode(s2, convert_to_tensor=True)
    return util.cos_sim(embeddings1, embeddings2).item()

def check_quality(original, new_text):
    if not new_text or len(new_text.strip()) == 0:
        return False, "ç©ºç»“æœ"
    
    # 1. é•¿åº¦æ£€æŸ¥
    len_ratio = len(new_text) / len(original)
    if len_ratio < MIN_LEN_RATIO: return False, f"å¤ªçŸ­ ({len_ratio:.2f})"
    if len_ratio > MAX_LEN_RATIO: return False, f"å¤ªé•¿ ({len_ratio:.2f})"

    # 2. å­—é¢ç›¸ä¼¼åº¦
    text_sim = calculate_text_similarity(original, new_text)
    if text_sim > MAX_TEXT_SIMILARITY:
        return False, f"å­—é¢å¤ªåƒåŸå¥ ({text_sim:.2f})"

    # 3. è¯­ä¹‰ç›¸ä¼¼åº¦
    sem_sim = calculate_semantic_similarity(original, new_text)
    if sem_sim < MIN_SEMANTIC_SIMILARITY:
        return False, f"æ„æ€åå·® ({sem_sim:.2f})"

    return True, f"é€šè¿‡ (è¯­ä¹‰:{sem_sim:.2f})"

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def generate_backtrans_json(original_text):
    """
    è®©æ¨¡å‹æ¨¡æ‹Ÿå¤šè¯­è¨€å›è¯‘è¿‡ç¨‹ï¼Œç›´æ¥è¿”å›æœ€ç»ˆä¸­æ–‡ç»“æœ
    """
    system_content = "ä½ æ˜¯ä¸€ä¸ªç²¾é€šå¤šå›½è¯­è¨€çš„ç¿»è¯‘ä¸“å®¶ã€‚è¯·ä¸¥æ ¼è¾“å‡º JSON æ ¼å¼ã€‚"
    
    prompt = f"""
    è¯·å¯¹ä»¥ä¸‹ä¸­æ–‡å¥å­è¿›è¡Œ 3 ç§ä¸åŒè·¯å¾„çš„â€œå›è¯‘â€ï¼ˆBack-Translationï¼‰ï¼Œä»¥è·å¾—å¤šæ ·åŒ–çš„ä¸­æ–‡è¡¨è¾¾ã€‚
    
    åŸå§‹ä¸­æ–‡ï¼š"{original_text}"

    è¯·æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼ˆåœ¨å†…éƒ¨æ€è€ƒï¼Œåªè¾“å‡ºæœ€ç»ˆçš„ä¸­æ–‡ç»“æœï¼‰ï¼š
    1. è·¯å¾„Aï¼šä¸­æ–‡ -> è‹±æ–‡ -> ä¸­æ–‡
    2. è·¯å¾„Bï¼šä¸­æ–‡ -> å¾·æ–‡ -> ä¸­æ–‡ (åˆ©ç”¨å¾·è¯­è¯­åºå·®å¼‚é‡ç»„å¥å­)
    3. è·¯å¾„Cï¼šä¸­æ–‡ -> æ—¥æ–‡ -> ä¸­æ–‡ (åˆ©ç”¨æ—¥è¯­è¯­å¢ƒå·®å¼‚é‡æ„å¥å­)

    è¦æ±‚ï¼š
    - æœ€ç»ˆè¾“å‡ºçš„ä¸­æ–‡å¿…é¡»é€šé¡ºã€è‡ªç„¶ã€‚
    - æ„æ€å¿…é¡»ä¸åŸå¥å®Œå…¨ä¸€è‡´ï¼ˆå› ä¸ºè¦å¯¹åº”æ‰‹è¯­ï¼‰ã€‚
    - å°½é‡ä¸åŸå¥çš„å­—é¢æªè¾æœ‰æ‰€ä¸åŒã€‚

    ã€è¯·è¾“å‡º JSON æ ¼å¼ã€‘ï¼š
    {{
        "variants": [
            {{"zh": "è·¯å¾„Açš„ç»“æœ", "path": "zh-en-zh"}},
            {{"zh": "è·¯å¾„Bçš„ç»“æœ", "path": "zh-de-zh"}},
            {{"zh": "è·¯å¾„Cçš„ç»“æœ", "path": "zh-ja-zh"}}
        ]
    }}
    """
    
    try:
        response = client.chat.completions.create(
            model=active_config['model_id'],
            messages=[
                {"role": "system", "content": system_content},
                {"role": "user", "content": prompt}
            ],
            **active_config['params']
        )
        content = response.choices[0].message.content.strip()
        
        # --- æµ‹è¯•æ¨¡å¼ï¼šæ‰“å°åŸå§‹è¾“å‡º ---
        if TEST_MODE:
            print(f"\n[TEST] åŸå¥: {original_text}")
            print(f"[TEST] æ¨¡å‹è¿”å›: {content}\n")
        # ---------------------------

        try:
            clean_content = re.sub(r'```json\s*|\s*```', '', content)
            data = json.loads(clean_content)
            return data.get("variants", [])
        except json.JSONDecodeError:
            logging.error(f"JSONè§£æå¤±è´¥: {content}")
            return []
            
    except Exception as e:
        raise e 

def process_single_row(index, row):
    original_zh = row['chinese']
    original_hksl = row['hksl']
    
    generated_results = []
    rejected_logs = []
    
    try:
        # è°ƒç”¨å¤šè¯­è¨€å›è¯‘
        variants_data = generate_backtrans_json(original_zh)
        
        for item in variants_data:
            new_zh = item.get('zh', '').strip()
            path_type = item.get('path', 'unknown')
            
            if not new_zh: continue

            # è´¨é‡æ£€æµ‹
            is_valid, reason = check_quality(original_zh, new_zh)
            
            if is_valid:
                generated_results.append({
                    'chinese': new_zh,
                    'hksl': original_hksl, 
                    'type': f'backtrans_{path_type}' 
                })
            else:
                rejected_logs.append({
                    'original': original_zh,
                    'generated': new_zh,
                    'path': path_type,
                    'reason': reason
                })
                
    except Exception as e:
        logging.error(f"è¡Œ {index} å¤„ç†å¤±è´¥: {e}")
            
    return generated_results, rejected_logs

# ================= 5. ä¸»ç¨‹åº =================

def main():
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {INPUT_FILE}")
        return

    # 1. è¯»å–åŸå§‹æ–‡ä»¶
    df = pd.read_csv(INPUT_FILE)
    print(f"ğŸ“„ è¯»å–æ–‡ä»¶æˆåŠŸï¼Œæ€»è¡Œæ•°: {len(df)}")
    
    # 2. ã€å…³é”®ä¿®æ”¹ã€‘åªç­›é€‰ type ä¸º 'original' çš„è¡Œ
    # è¿™æ ·å¯ä»¥ä¿è¯åªå¯¹åŸå¥åšå¢å¼ºï¼Œä¸ä¼šå¯¹å·²ç»æ˜¯ augm çš„æ•°æ®åšäºŒæ¬¡å¤„ç†
    if 'type' not in df.columns:
        print("âŒ é”™è¯¯ï¼šCSVæ–‡ä»¶ä¸­ç¼ºå°‘ 'type' åˆ—ï¼Œæ— æ³•ç­›é€‰ original æ•°æ®ã€‚")
        return

    originals_df = df[df['type'] == 'original'].copy()
    print(f"ğŸ” ç­›é€‰å‡º 'original' æ•°æ®: {len(originals_df)} æ¡")

    # 3. æ ¹æ®æ˜¯å¦æµ‹è¯•æ¨¡å¼ï¼Œç¡®å®šæœ€ç»ˆè¦å¤„ç†çš„æ•°æ® target_df
    if TEST_MODE:
        target_df = originals_df.head(TEST_LIMIT).copy()
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šä»…å¤„ç†å‰ {len(target_df)} æ¡æ•°æ®")
    else:
        target_df = originals_df.copy()
        print(f"ğŸš€ æ­£å¼æ¨¡å¼ï¼šå°†å¤„ç†æ‰€æœ‰ {len(target_df)} æ¡ original æ•°æ®")
    
    if len(target_df) == 0:
        print("âš ï¸ æ²¡æœ‰éœ€è¦å¤„ç†çš„æ•°æ®ï¼Œç¨‹åºç»“æŸã€‚")
        return

    all_new_rows = []
    all_rejected = []
    
    # 4. å¼€å§‹å¤šçº¿ç¨‹å¤„ç†
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_index = {
            executor.submit(process_single_row, idx, row): idx 
            for idx, row in target_df.iterrows()
        }
        
        if TEST_MODE:
            print("ğŸ‘€ æµ‹è¯•æ¨¡å¼ä¸‹ç›´æ¥è¾“å‡ºæ—¥å¿—...")
            for future in as_completed(future_to_index):
                try:
                    new_rows, rejected = future.result()
                    all_new_rows.extend(new_rows)
                    all_rejected.extend(rejected)
                except Exception as exc:
                    logging.error(f"ä»»åŠ¡å¼‚å¸¸: {exc}")
        else:
            for future in tqdm(as_completed(future_to_index), total=len(target_df), desc="å›è¯‘ä¸­"):
                try:
                    new_rows, rejected = future.result()
                    all_new_rows.extend(new_rows)
                    all_rejected.extend(rejected)
                except Exception as exc:
                    logging.error(f"ä»»åŠ¡å¼‚å¸¸: {exc}")

    # 5. ä¿å­˜ç»“æœ
    print(f"\nğŸ“Š ç»Ÿè®¡:")
    print(f"  - è¾“å…¥æ€»æ•°æ®: {len(df)} æ¡")
    print(f"  - æœ¬æ¬¡å¤„ç†æºæ•°æ®: {len(target_df)} æ¡")
    print(f"  - æ–°å¢å›è¯‘æ•°æ®: {len(all_new_rows)} æ¡")
    print(f"  - è¿‡æ»¤æ‰çš„æ•°æ®: {len(all_rejected)} æ¡")

    if all_new_rows:
        df_new = pd.DataFrame(all_new_rows)
        
        # ã€å…³é”®åˆå¹¶ã€‘
        # å°† "åŸå§‹çš„å®Œæ•´æ•°æ®(df)" å’Œ "æ–°ç”Ÿæˆçš„æ•°æ®(df_new)" æ‹¼åœ¨ä¸€èµ·
        # è¿™æ ·æ—¢ä¿ç•™äº† original åŸå¥ï¼Œä¹Ÿä¿ç•™äº†æ—§çš„ augm æ•°æ®ï¼Œåˆå¢åŠ äº†æ–°ç”Ÿæˆçš„å¥å­
        df_final = pd.concat([df, df_new], ignore_index=True)
            
        df_final.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')
        print(f"ğŸ‰ ç»“æœå·²ä¿å­˜è‡³æ–°æ–‡ä»¶: {OUTPUT_FILE}")
    else:
        print("âš ï¸ æœ¬æ¬¡æ²¡æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆçš„æ–°æ•°æ®ã€‚")

    if all_rejected:
        pd.DataFrame(all_rejected).to_csv(REJECTED_FILE, index=False, encoding='utf-8-sig')
        print(f"ğŸ“ æ‹’ç»è®°å½•å·²ä¿å­˜: {REJECTED_FILE}")

if __name__ == "__main__":
    main()